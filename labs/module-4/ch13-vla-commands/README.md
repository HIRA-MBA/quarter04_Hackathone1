# Lab 13: Vision-Language-Action Models

## Overview

Integrate VLA models for natural language robot control.

## Prerequisites

- Completed Labs 11-12
- PyTorch experience

## Learning Outcomes

- Load VLA models
- Parse language commands
- Map vision to actions
- Deploy on robot hardware

## Files

- `vla_agent.py` - VLA integration
- `language_parser.py` - NL processing
- `INSTRUCTIONS.md` - Step-by-step guide

## Expected Duration

3-4 hours
